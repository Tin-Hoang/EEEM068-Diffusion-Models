{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077d06e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports and Config\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "from diffusers import DiTModel, DDPMPipeline, DDPMScheduler\n",
    "from accelerate import Accelerator\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from piq import FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dd0d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Training Configuration\n",
    "class TrainingConfig:\n",
    "    image_size = 256\n",
    "    train_batch_size = 2\n",
    "    learning_rate = 1e-4\n",
    "    num_epochs = 10\n",
    "    output_dir = \"dit_celebahq_model\"\n",
    "    mixed_precision = \"fp16\"  # or \"no\" for float32\n",
    "    wandb_project = \"diffusion-dit-celebahq\"\n",
    "\n",
    "config = TrainingConfig()\n",
    "\n",
    "# 3. Init W&B\n",
    "wandb.init(project=config.wandb_project, config=vars(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bc9ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Load Dataset (CelebA-HQ stored locally or mounted)\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=\"celeba_hq_256\", split=\"train\")\n",
    "\n",
    "# 5. Preprocess Images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((config.image_size, config.image_size)),\n",
    "    transforms.CenterCrop(config.image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5]),\n",
    "])\n",
    "\n",
    "def transform_fn(examples):\n",
    "    images = [transform(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return {\"images\": images}\n",
    "\n",
    "dataset.set_transform(transform_fn)\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f7ed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Load Pretrained DiT-S Model\n",
    "model = DiTModel.from_pretrained(\"facebook/DiT-S-256\", subfolder=\"model\")\n",
    "scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "pipeline = DDPMPipeline(unet=model, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946dc2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Prepare for Training\n",
    "accelerator = Accelerator(mixed_precision=config.mixed_precision)\n",
    "model, train_dataloader = accelerator.prepare(model, train_dataloader)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "fid_metric = FID().to(accelerator.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cddd5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Training Loop\n",
    "model.train()\n",
    "for epoch in range(config.num_epochs):\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        clean_images = batch[\"images\"].to(accelerator.device)\n",
    "        noise = torch.randn_like(clean_images)\n",
    "        timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (clean_images.shape[0],), device=clean_images.device).long()\n",
    "\n",
    "        noisy_images = scheduler.add_noise(clean_images, noise, timesteps)\n",
    "        noise_pred = model(noisy_images, timesteps).sample\n",
    "\n",
    "        loss = torch.nn.functional.mse_loss(noise_pred, noise)\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "        wandb.log({\"train/loss\": loss.item()})\n",
    "\n",
    "    # Generate samples for FID after each epoch\n",
    "    model.eval()\n",
    "    generated_images = []\n",
    "    real_images = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            with torch.autocast(\"cuda\"):\n",
    "                samples = pipeline(num_inference_steps=50).images\n",
    "            for img in samples:\n",
    "                tensor_img = transform(img).unsqueeze(0).to(accelerator.device)\n",
    "                generated_images.append(tensor_img)\n",
    "        for real_batch in train_dataloader:\n",
    "            real_images.extend(real_batch[\"images\"].to(accelerator.device).unsqueeze(0))\n",
    "            if len(real_images) >= len(generated_images):\n",
    "                break\n",
    "    fake_tensor = torch.cat(generated_images[:len(real_images)])\n",
    "    real_tensor = torch.cat(real_images[:len(generated_images)])\n",
    "    fid_score = fid_metric(fake_tensor, real_tensor).item()\n",
    "    wandb.log({\"eval/fid\": fid_score})\n",
    "\n",
    "    # Save model after each epoch\n",
    "    if accelerator.is_main_process:\n",
    "        model.save_pretrained(f\"{config.output_dir}_epoch{epoch+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4618a366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Inference Example\n",
    "model.eval()\n",
    "pipeline = DDPMPipeline(unet=model, scheduler=scheduler).to(\"cuda\")\n",
    "with torch.autocast(\"cuda\"):\n",
    "    images = pipeline(num_inference_steps=50).images\n",
    "\n",
    "images[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57579551",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
