# Model configuration
model: latent_conditional_unet
run_name: attribute_latentconditionalunet2d_256_fp32_162ktrain

# Training configuration
image_size: 256
train_batch_size: 16
eval_batch_size: 16
num_epochs: 100
learning_rate: 1e-4
weight_decay: 1e-2
lr_warmup_steps: 5000
gradient_accumulation_steps: 1
seed: 42
mixed_precision: "no"

# Save and logging configuration
save_image_epochs: 5
save_model_epochs: 5
root_output_dir: /scratch/group_5/diffusion_checkpoints

# Dataset configuration
train_dir: data/CelebA-Large-split/train_162k
val_dir: null
val_n_samples: null
dataset_name: celeba_large_256
num_workers: 4

# Diffusion configuration
num_train_timesteps: 1000
scheduler_type: ddpm

# WandB configuration
use_wandb: true
wandb_project: EEEM068_Diffusion_Models
wandb_entity: tin-hoang

# Model architecture configuration
use_ema: false
use_scale_shift_norm: false

# Conditional generation configuration
is_conditional: true
attribute_file: data/CelebA-Large-split/Anno/list_attr_celeba.txt
num_attributes: 40
grid_attribute_indices: [35]  # Wearing Hat
grid_num_samples: 16
grid_sample_random_remaining_indices: true
